{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation from Raw PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 12.3/16.6 MB 71.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 60.7 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.4\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to ./data/extracted_pdf.json\n",
      "Processed fine-tuning dataset saved to ./data/fine_tuning_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_json):\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF and saves it in a JSON file.\n",
    "    Each key is formatted as \"page_X\" where X is the page number.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pdf_content = {}\n",
    "    for page_number in range(doc.page_count):\n",
    "        page = doc[page_number]\n",
    "        text = page.get_text()\n",
    "        pdf_content[f\"page_{page_number + 1}\"] = text\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pdf_content, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Extracted text saved to {output_json}\")\n",
    "\n",
    "def remove_headers_footers(text):\n",
    "    \"\"\"\n",
    "    Remove lines that are likely headers or footers, such as page numbers\n",
    "    or lines that match \"Page X\" patterns.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Skip lines that contain only numbers\n",
    "        if re.match(r'^\\s*\\d+\\s*$', line):\n",
    "            continue\n",
    "        # Skip lines matching patterns like \"Page 1\" (case-insensitive)\n",
    "        if re.match(r'^\\s*Page\\s+\\d+\\s*$', line, re.IGNORECASE):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines).strip()\n",
    "\n",
    "def clean_special_characters(text):\n",
    "    \"\"\"\n",
    "    Normalize unicode characters and remove non-printable characters.\n",
    "    Also collapses multiple spaces/newlines.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[^\\x20-\\x7E]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_words=100):\n",
    "    \"\"\"\n",
    "    Split text into smaller chunks if it exceeds max_words.\n",
    "    Adjust max_words based on your model's input size limitations.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = \" \".join(words[i:i + max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_pdf_json(input_file, output_file, max_words_per_chunk=100):\n",
    "    \"\"\"\n",
    "    Loads the extracted PDF JSON, cleans the text, and splits long pages into chunks.\n",
    "    Then, it formats each chunk as a dictionary with a \"text\" key and saves the data.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        pdf_data = json.load(f)\n",
    "    \n",
    "    training_examples = []\n",
    "    \n",
    "    for key, raw_text in pdf_data.items():\n",
    "        # Remove headers/footers\n",
    "        cleaned_text = remove_headers_footers(raw_text)\n",
    "        # Clean special characters and extra whitespace\n",
    "        cleaned_text = clean_special_characters(cleaned_text)\n",
    "        # Split the cleaned text into chunks if necessary\n",
    "        chunks = chunk_text(cleaned_text, max_words=max_words_per_chunk)\n",
    "        # Add each chunk as a separate training example\n",
    "        for chunk in chunks:\n",
    "            training_examples.append({\"text\": chunk})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_examples, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Processed fine-tuning dataset saved to {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File paths (keep the file names as requested)\n",
    "    pdf_path = './data/augmentation.pdf'\n",
    "    extracted_json = './data/extracted_pdf.json'\n",
    "    fine_tuning_json = './data/fine_tuning_dataset.json'\n",
    "    \n",
    "    # Step 1: Extract text from the PDF and save it as JSON\n",
    "    extract_text_from_pdf(pdf_path, extracted_json)\n",
    "    \n",
    "    # Step 2: Process the extracted JSON and prepare fine-tuning data\n",
    "    process_pdf_json(extracted_json, fine_tuning_json, max_words_per_chunk=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
      "  Cloning https://github.com/huggingface/transformers (to revision 46350f5eae87ac1d168ddfdc57a0b39b64b9a029) to c:\\users\\lucasmartins\\appdata\\local\\temp\\pip-install-u1498rba\\transformers_33d1cc310d5b4d8682ac396f2b1d4d77\n",
      "  Resolved https://github.com/huggingface/transformers to commit 46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (0.29.3)\n",
      "Collecting numpy>=1.17 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->transformers@ git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029) (2025.1.31)\n",
      "Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 3.1/12.6 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.6 MB 20.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 19.0 MB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 17.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.50.0.dev0-py3-none-any.whl size=10936767 sha256=bf6fb3db9d966bf34f08e23266c741fa1cb96d01b20e4299aa2dc7f5eee4a9a7\n",
      "  Stored in directory: c:\\users\\lucasmartins\\appdata\\local\\pip\\cache\\wheels\\c3\\e4\\b1\\c918fc6f31e83369415e4e3016f7638615818889ea75964cc0\n",
      "Successfully built transformers\n",
      "Installing collected packages: safetensors, regex, numpy, tokenizers, transformers\n",
      "Successfully installed numpy-2.2.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-u1498rba\\transformers_33d1cc310d5b4d8682ac396f2b1d4d77'\n",
      "  Running command git rev-parse -q --verify 'sha^46350f5eae87ac1d168ddfdc57a0b39b64b9a029'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers 46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
      "  Running command git checkout -q 46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub==0.29.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub==0.29.3) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.29.3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface-hub==0.29.3) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers@git+https://github.com/huggingface/transformers@46350f5eae87ac1d168ddfdc57a0b39b64b9a029\n",
    "!pip install huggingface-hub==0.29.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from trl) (1.5.2)\n",
      "Collecting datasets>=2.21.0 (from trl)\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: transformers>=4.46.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from trl) (4.50.0.dev0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from datasets>=2.21.0->trl) (3.18.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.21.0->trl)\n",
      "  Using cached pyarrow-19.0.1-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.21.0->trl)\n",
      "  Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Collecting xxhash (from datasets>=2.21.0->trl)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets>=2.21.0->trl)\n",
      "  Downloading aiohttp-3.11.14-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Downloading multidict-6.2.0-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached propcache-0.3.0-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.21.0->trl)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets>=2.21.0->trl) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.21.0->trl)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.21.0->trl)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp313-cp313-win_amd64.whl (436 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-19.0.1-cp313-cp313-win_amd64.whl (25.2 MB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-win_amd64.whl (51 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.2.0-cp313-cp313-win_amd64.whl (28 kB)\n",
      "Using cached propcache-0.3.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-win_amd64.whl (315 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, aiosignal, rich, aiohttp, datasets, trl\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 attrs-25.3.0 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.2.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 rich-13.9.4 trl-0.15.2 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Requirement already satisfied: peft in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (4.50.0.dev0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install trl\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LucasMartins\\.cache\\huggingface\\hub\\models--google--gemma-3-4b-pt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 2/2 [25:42<00:00, 771.13s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load the Gemma 3 model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mGemma3ForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Disable caching for training\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Set up LoRA configuration for causal language modeling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:273\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    275\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4531\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4528\u001b[39m         device_map_kwargs[\u001b[33m\"\u001b[39m\u001b[33moffload_buffers\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   4530\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m4531\u001b[39m         \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4533\u001b[39m \u001b[38;5;66;03m# This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\u001b[39;00m\n\u001b[32m   4534\u001b[39m \u001b[38;5;66;03m# not part of the state_dict (persistent=False)\u001b[39;00m\n\u001b[32m   4535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\accelerate\\big_modeling.py:501\u001b[39m, in \u001b[36mdispatch_model\u001b[39m\u001b[34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[39m\n\u001b[32m    499\u001b[39m     device = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mxpu:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device != \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3263\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3259\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3260\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3261\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3262\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1336\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1337\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1338\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhen moving module from meta to a different device.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1339\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "#test Gemma3\n",
    "\n",
    "from transformers import AutoTokenizer, TrainingArguments, Gemma3ForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset (for example purposes, we use wikitext-2)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"google/gemma-3-4b-pt\"\n",
    "\n",
    "# Load the tokenizer and adjust padding settings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos token as pad token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the Gemma 3 model\n",
    "model = Gemma3ForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "model.config.use_cache = False  # Disable caching for training\n",
    "\n",
    "# Set up LoRA configuration for causal language modeling\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    save_steps=25,\n",
    "    report_to=\"tensorboard\",\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "# Create the SFTTrainer with LoRA parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gemma3_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 27.4 MB/s eta 0:00:00\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1\n",
      "Requirement already satisfied: nlpaug in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from nlpaug) (2.2.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from nlpaug) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from nlpaug) (2.32.3)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucasmartins\\documents\\llm_garage\\.venv\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install nlpaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It was a dark and A6ormy nKghh. I was ao)ne at none 3heM I saw a li)m ' s face f)ilowDd by a scaF6 tGundero7D roq% at the windows.\"]\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "text = \"It was a dark and stormy night. I was alone at home when I saw a lion's face followed by a scary thunderous roar at the windows.\"\n",
    "\n",
    "# Keyboard \n",
    "aug = nac.KeyboardAug()\n",
    "augmented_text = aug.augment(text)\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3.tar.gz (23.3 MB)\n",
      "     ---------------------------------------- 0.0/23.3 MB ? eta -:--:--\n",
      "     --------------- ------------------------ 8.9/23.3 MB 58.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 23.3/23.3 MB 66.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
      "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "     ------------------------------------ -- 14.7/15.8 MB 76.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.8/15.8 MB 66.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      + c:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Scripts\\python.exe C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327\\vendored-meson\\meson\\meson.py setup C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327 C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327\\.mesonpy-0jmijdz2 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327\\.mesonpy-0jmijdz2\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327\n",
      "      Build dir: C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327\\.mesonpy-0jmijdz2\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not parse vswhere.exe output\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\LucasMartins\\AppData\\Local\\Temp\\pip-install-4lnai3kz\\numpy_7af841e68a914205a3283bca59bc9327\\.mesonpy-0jmijdz2\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=5ffafb98-283e-4b9b-ba52-7a77c770d422\n",
      "To: c:\\Users\\LucasMartins\\Documents\\llm_garage\\GoogleNews-vectors-negative300.bin.gz\n",
      "100%|██████████| 1.65G/1.65G [00:26<00:00, 61.0MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlpaug\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DownloadUtil\n\u001b[32m      2\u001b[39m DownloadUtil.download_word2vec(dest_dir=\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Download word2vec model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mDownloadUtil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_glove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mglove.6B\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Download GloVe model\u001b[39;00m\n\u001b[32m      4\u001b[39m DownloadUtil.download_fasttext(model_name=\u001b[33m'\u001b[39m\u001b[33mwiki-news-300d-1M\u001b[39m\u001b[33m'\u001b[39m, dest_dir=\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Download fasttext model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nlpaug\\util\\file\\download.py:66\u001b[39m, in \u001b[36mDownloadUtil.download_glove\u001b[39m\u001b[34m(model_name, dest_dir)\u001b[39m\n\u001b[32m     56\u001b[39m     possible_values = [\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mglove.6B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mglove.42B.300d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mglove.840B.300d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mglove.twitter.27B\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     ]\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     63\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnknown model_name. Possible values are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(possible_values)\n\u001b[32m     64\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m file_path = \u001b[43mDownloadUtil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdest_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m DownloadUtil.unzip(file_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nlpaug\\util\\file\\download.py:116\u001b[39m, in \u001b[36mDownloadUtil.download\u001b[39m\u001b[34m(src, dest_dir, dest_file)\u001b[39m\n\u001b[32m    114\u001b[39m     file = urllib.request.urlopen(req)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(dest_dir, dest_file), \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         output.write(\u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m os.path.join(dest_dir, dest_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:495\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m         s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[32m    497\u001b[39m         \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:642\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[32m    636\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[32m    637\u001b[39m \n\u001b[32m    638\u001b[39m \u001b[33;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[33;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m    644\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.752.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from nlpaug.util.file.download import DownloadUtil\n",
    "DownloadUtil.download_word2vec(dest_dir='.') # Download word2vec model\n",
    "DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.') # Download GloVe model\n",
    "DownloadUtil.download_fasttext(model_name='wiki-news-300d-1M', dest_dir='.') # Download fasttext model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LucasMartins\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LucasMartins/nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LucasMartins\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nlpaug\\model\\word_dict\\wordnet.py:58\u001b[39m, in \u001b[36mWordNet.pos_tag\u001b[39m\u001b[34m(cls, tokens)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     results = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03mtag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LucasMartins/nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LucasMartins\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Synonym\u001b[39;00m\n\u001b[32m      6\u001b[39m aug = naw.SynonymAug(aug_src=\u001b[33m'\u001b[39m\u001b[33mwordnet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m augmented_text = \u001b[43maug\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(augmented_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nlpaug\\base_augmenter.py:119\u001b[39m, in \u001b[36mAugmenter.augment\u001b[39m\u001b[34m(self, data, n, num_thread)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Single input with/without multiple input\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Single Thread\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_thread == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         augmented_results = [\u001b[43maction_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# Multi Thread\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    123\u001b[39m         augmented_results = \u001b[38;5;28mself\u001b[39m._parallel_augment(action_fx, clean_data, n=n, num_thread=num_thread)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nlpaug\\augmenter\\word\\synonym.py:121\u001b[39m, in \u001b[36mSynonymAug.substitute\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    117\u001b[39m doc = Doc(data, \u001b[38;5;28mself\u001b[39m.tokenizer(data))\n\u001b[32m    119\u001b[39m original_tokens = doc.get_original_tokens()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m pos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m aug_idxes = \u001b[38;5;28mself\u001b[39m._get_aug_idxes(pos)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m aug_idxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(aug_idxes) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nlpaug\\model\\word_dict\\wordnet.py:61\u001b[39m, in \u001b[36mWordNet.pos_tag\u001b[39m\u001b[34m(cls, tokens)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m     60\u001b[39m     nltk.download(\u001b[33m'\u001b[39m\u001b[33maveraged_perceptron_tagger\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     results = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LucasMartins\\Documents\\llm_garage\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LucasMartins/nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\LucasMartins\\\\Documents\\\\llm_garage\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LucasMartins\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "text = \"It was a dark and stormy night. I was alone at home when I saw a lion's face followed by a scary thunderous roar at the windows.\"\n",
    "\n",
    "# Synonym\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/augment', methods=['POST'])\n",
    "def augment():\n",
    "    \"\"\"\n",
    "    API endpoint to return the augmented dataset as JSON.\n",
    "    \"\"\"\n",
    "    # Load the augmented dataset from the file\n",
    "    augmented_dataset_path = './data/fine_tuning_dataset.json'\n",
    "    with open(augmented_dataset_path, 'r', encoding='utf-8') as f:\n",
    "        augmented_data = json.load(f)\n",
    "    \n",
    "    # Return the augmented dataset as a JSON response\n",
    "    return jsonify(augmented_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
